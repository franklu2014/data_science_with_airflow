{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apache Airflow for building pipeline\n",
    "In the field of Data Science, a data pipeline usually represents a system that ingests, processes, analyzes data.  Sometimes a pipeline also fits and evaluates machine learning models and produce predictions.  As I have came across quite a bit of job postings that ask for experience in building data pipelines with Apache Airflow, I decided to give it a try and increase my own competency.  \n",
    "\n",
    "Apache Airflow is an open source pipeline building library that all the coding can be based on Python.  Since I mostly practice data science in Python, building pipelines with Airflow becomes a great option for me."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project goal\n",
    "As the purpose is building a data pipeline, I want to dig out something interesting from open data source.  As social media has been a major way of communications, my idea is to cross-analyze Tweeter messages and other societal or environmental factors.\n",
    "\n",
    "For example, it'll be interesting to see if human communications are affected by the weather, or if results of sports games spur more discussion than government policies do.\n",
    "\n",
    "As this idea involves multiple data sources and various areas in machine learning, I try to lay out the plan of attack in different phases:  \n",
    "- Phase 1: Ingest and aggregate weather data.  \n",
    "    I'm planning to use the API of OpenWeatherMap to get region specific weather data.  Thankfully, OpenWeatherMap provides free-tier of service that allows 1M api calls per month, which suits my requirement very well\n",
    "    \n",
    "- Phase 2: Ingest Tweeter data and perform topic modeling.  \n",
    "    Tweeter also provides free-tier API registration and is suitable for my needs.  For now, I am planning to do topics modeling using Gensim to find the trend of subject matters in people's tweets.  \n",
    "    \n",
    "- Phase 3: Ingest news feeds and perform topic modeling.  \n",
    "    As I'll start by focusing on Canadian news, CBC has RSS feeds to be used.  I am interested in comparing the popular topics of Canadian news with the hot topics on Tweeter.\n",
    "\n",
    "- Phase 4: Semantic analysis on Tweeter messages.  \n",
    "    After the trend has been found, I am also interested in finding whether semantics of Tweeter messages do get affected by the weather and the news."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 1\n",
    "Let's start by building the skeleton of a functional data pipeline for ingesting and processing weather data:\n",
    "- Apache Airflow can be installed by entering `conda install -c conda-forge airflow` in the terminal\n",
    "- Airflow provides an [official introduction to some basic concept of pipeline construction](https://airflow.apache.org/docs/stable/tutorial.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> First, we create a file called `AIRFLOW_HOME/dags/my_dag.py`.  In the beginning, import some useful libraries.\n",
    "```python\n",
    "from airflow import DAG\n",
    "from airflow.operators.python_operator import PythonOperator\n",
    "from airflow.operators.bash_operator import BashOperator\n",
    "\n",
    "from datetime import datetime, date, time\n",
    "from datetime import timedelta\n",
    "\n",
    "import requests\n",
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import altair as alt\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Create dictionaries for default arguments and cities I'm interested in.  Here we have only 3 cities for easier troubleshooting.  \n",
    "The best part is that the arguments can be Python objects\n",
    "```python\n",
    "default_args = {\n",
    "    'owner': 'airflow',\n",
    "    'depends_on_past': False,\n",
    "    'start_date': datetime(2020, 9, 1),\n",
    "    'email_on_failure': False,\n",
    "    'email_on_retry': False,\n",
    "    'retries': 1,\n",
    "    'retry_delay': timedelta(seconds = 5)\n",
    "}\n",
    "cityIds = {\n",
    "    'Burnaby': 5911606,\n",
    "    'Vancouver': 6173331,\n",
    "    'Richmond': 6122085\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Define the first task for ingesting, processing and saving the most current weather data.  `OWM_KEY` is provided upon completing the registration with OpenWeatherMap.\n",
    "```python\n",
    "def task1():\n",
    "    city, timestamp, condition, temperature, humidity, visibility, cloudiness = \\\n",
    "        list(), list(), list(), list(), list(), list(), list()\n",
    "    absZero = -273.15\n",
    "    t = datetime.now()\n",
    "\n",
    "    for name, cityId in cityIds.items():\n",
    "        w = requests.get(f\"http://api.openweathermap.org/data/2.5/weather?id={cityId}&appid={os.environ['OWM_KEY']}\") \\\n",
    "            .json()\n",
    "        city.append(w['name'])\n",
    "        timestamp.append(t)\n",
    "        condition.append(w['weather'][0]['main'].lower())\n",
    "        temperature.append(w['main']['temp'] + absZero)\n",
    "        humidity.append(w['main']['humidity'])\n",
    "        visibility.append(w['visibility'])\n",
    "        cloudiness.append(w['clouds']['all'])\n",
    "    weathers = pd.DataFrame({\n",
    "            'city': city,\n",
    "            'timestamp': timestamp,\n",
    "            'condition': condition, \n",
    "            'temperature': temperature, \n",
    "            'humidity': humidity, \n",
    "            'visibility': visibility, \n",
    "            'cloudiness': cloudiness\n",
    "        })\n",
    "    weathers.to_csv(f\"weathers_{t.date()}T{t.hour:02}-{t.minute:02}-{t.second:02}.csv\", \n",
    "                    index = False)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Task 2 reads the most current weather data from the disk and combine it with the historical one.  \n",
    "```python\n",
    "def read_csv_timestamp(path, cols = ['timestamp']):\n",
    "    df = pd.read_csv(path)\n",
    "    for col in cols:\n",
    "        df[col] = df[col].apply(lambda x: datetime.fromisoformat(x))\n",
    "    return df\n",
    "\n",
    "def task2():\n",
    "    latestCsv = sorted([x for x in os.listdir() if re.findall(r\"^weathers.*\\.csv$\", x)])[-1]\n",
    "    newWeathers = read_csv_timestamp(latestCsv)\n",
    "    if not os.path.exists('all_weathers.csv'):\n",
    "        allWeathers = pd.DataFrame({\n",
    "                'city': list(),\n",
    "                'timestamp': list(),\n",
    "                'condition': list(), \n",
    "                'temperature': list(), \n",
    "                'humidity': list(), \n",
    "                'visibility': list(), \n",
    "                'cloudiness': list()\n",
    "            })\n",
    "    else:\n",
    "        allWeathers = read_csv_timestamp(\"all_weathers.csv\")\n",
    "    allWeathers = allWeathers.append(newWeathers, ignore_index = True)\n",
    "    allWeathers.to_csv(\"all_weathers.csv\", index = False)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Task 3 loads the comprehensive weather data and save the visualization as PNG.  Note that the saved PNG uses the pipeline timestamp as part of the filename. Because the pipeline scheduler coordinate the work based on the schedules defined by `start_date`, the pipeline timestamp can be different from the system time.\n",
    "```python\n",
    "def task3(ds, ts, **kwargs):\n",
    "    allWeathers = read_csv_timestamp(\"all_weathers.csv\")\n",
    "    allWeathers['date'] = allWeathers['timestamp'].apply(lambda x: x.date())\n",
    "    allWeathers['time'] = allWeathers['timestamp'].apply(lambda x: x.time())\n",
    "    allWeathers['time'] = allWeathers['time'].apply(lambda x: x.isoformat('seconds'))\n",
    "    base = alt.Chart(allWeathers.drop(columns = ['date'])).encode(\n",
    "        alt.X('time:N')\n",
    "    )\n",
    "    bar = base.mark_bar().encode(\n",
    "        alt.Y(\"temperature:Q\"),\n",
    "        facet = alt.Facet('city:N')\n",
    "    )\n",
    "    line = base.mark_line(color = 'red').encode(\n",
    "        alt.Y(\"humidity:Q\",\n",
    "            scale = alt.Scale(zero = False))\n",
    "    ).facet(\n",
    "        alt.Facet('city:N'),\n",
    "        columns = 3\n",
    "    )\n",
    "    line.save(f'test_{ts}.png', scale_factor = 1.2)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> After the main functions are defined, now we set up the default DAG and corresponding operators to perform the tasks.  Note that `provide_context = True` in `op3` passes the scheduler timestamp to `task3()`.\n",
    "```python\n",
    "dag = DAG(\n",
    "    dag_id = 'my_dag',\n",
    "    description = 'First DAG',\n",
    "    default_args = default_args,\n",
    "    schedule_interval = timedelta(minutes = 10),\n",
    ")\n",
    "\n",
    "\n",
    "op1 = PythonOperator(\n",
    "    task_id = 'task1',\n",
    "    python_callable = task1,\n",
    "    dag = dag\n",
    ")\n",
    "\n",
    "op2 = PythonOperator(\n",
    "    task_id = 'task2',\n",
    "    python_callable = task2,\n",
    "    # provide_context = True,\n",
    "    dag = dag\n",
    ")\n",
    "\n",
    "op3 = PythonOperator(\n",
    "    task_id = 'task3',\n",
    "    python_callable = task3,\n",
    "    provide_context = True,\n",
    "    dag = dag\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Here is just a trivial example for showing that Airflow provides operators to execute commands in both Python and Bash.\n",
    "```python\n",
    "op_end_command = \"\"\"\n",
    "{% for i in range(5) %}\n",
    "    echo \"{{ macros.ds_add(ds, i) }}\"\n",
    "{% endfor %}\n",
    "\"\"\"\n",
    "\n",
    "op_end = BashOperator(\n",
    "    task_id = 'last_task',\n",
    "    bash_command = op_end_command,\n",
    "    dag = dag\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> At the end of `my_dag.py`, we define the sequence of operator execution.  The code below means: op1 -> op2 -> op3 -> op_end, which is a queued operation.  Later when the pipeline becomes more complicated, the operation sequnce should look more like a Directed Acyclic Graph.\n",
    "```python\n",
    "op1 >> op2\n",
    "op2 >> op3\n",
    "op3 >> op_end\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After `my_dag.py` is completed, we can bring up the pipeline.  At AIRFLOW_HOME directory:\n",
    "- run `airflow scheduler` to bring up the scheduler and start executing the tasks.\n",
    "- open a new terminal, run `airflow webserver -p 8080` to bring up the Web UI for monitoring the pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> If the pipeline is running well, you should see a page like below:  \n",
    "\n",
    "![](Airflow_UI.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> In the directory, the saved visualizations can also be found:\n",
    "\n",
    "![](saved_visualizations.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> I also verified that the PNG files correctly represent the aggregated weather data.  At this point, the pipeline is properly ingesting, processing, and visualizing the weather data automatically.  We can look into Phase 2: Tweeter ingestion now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
